#include "tflm_inference.h"
#include <stdio.h>

// TFLM integration notes:
// - This wrapper is intentionally minimal. To use TensorFlow Lite Micro:
//   1) Add the TFLM sources to your CubeIDE project or link prebuilt library.
//   2) Provide model data as a C array (use xxd -i on your .tflite).
//   3) Allocate a static tensor arena (size depends on model).
//
// Example pseudo-implementation provided so project compiles without TFLM if USE_TFLM not set.

#ifndef USE_TFLM
void tflm_init(void)
{
    // TFLM not enabled â€” nothing to do
}
int tflm_run_inference(uint8_t *input_gray, int w, int h)
{
    (void)input_gray; (void)w; (void)h;
    return -1;
}
#else
// If you set -DUSE_TFLM compile flag, implement real TFLM initialization here.
#include "model_data.h" // model blob produced by xxd -i
#include "tensorflow/lite/micro/all_ops_resolver.h"
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/schema/schema_generated.h"
#include "tensorflow/lite/version.h"
static uint8_t tensor_arena[64*1024]; // tune size for your model

void tflm_init(void)
{
    const tflite::Model *model = tflite::GetModel(model_data);
    static tflite::MicroMutableOpResolver<10> resolver;
    resolver.AddBuiltin(tflite::BuiltinOperator_DEPTHWISE_CONV_2D, tflite::ops::micro::Register_DEPTHWISE_CONV_2D());
    // Add other ops your model needs...
    static tflite::MicroInterpreter static_interpreter(model, resolver, tensor_arena, sizeof(tensor_arena));
    // perform allocations, etc.
}

int tflm_run_inference(uint8_t *input_gray, int w, int h)
{
    // Fill input tensor, invoke interpreter, parse output -> return label idx
    return -1;
}
#endif
